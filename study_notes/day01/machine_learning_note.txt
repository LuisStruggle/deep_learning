1，若我们欲预测的是离散值，例如“好瓜”，“坏瓜”，此类学习任务称为“分类”（classification）
若欲预测的事连续值，例如西瓜成熟度0.95、0.37，此类学习任务称为“回归”（regression）
我们还可以对西瓜做“聚类（clustering），即将训练集中的西瓜分成若干组，每组成为一个”簇“（cluster）；这些自动形成的簇可能对应一些潜在的概念划分，例如“深色瓜”，甚至“本地瓜” “外地瓜”；这样的学习过程中有助于我们了解数据内在的规律，能为更深入地分析数据建立基础；需要说明的是，在聚类学习中，“浅色瓜” “本地瓜”这样的概念我们事先是不知道的，而且学习过程中使用的训练样本通常不拥有标记信息

2，根据训练数据是否拥有标记信息，学习任务可以大致划分为两大类：“监督学习”（supervised learning）和“无监督学习”（unsupervised learning），分类和回归是前者的代表，而聚类则是后则的代表

3，机器学习的目标是使学得的模型能很好地适应于“新样本”，而不是仅仅在训练样本上工作得很好

4，任何一个有效的机器学习算法必有其归纳偏好，否则它将被假设空间中看似在训练集上“等效”的假设所迷惑，而无法产生确定的学习结果，可以想象，如果没有偏好，我们的西瓜学习算法产生的模型每次在进行预测时都是随机抽选训练集上的等效假设，学得模型时而好，时而不好，这样是没有意义的

5，NFL定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论“什么学习算法更好”毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好。学习算法自身的归纳偏好与问题是否相配，往往会起到决定性的作用

6，“从样例中学习”的一大主流是符号主义学习，其代表包括决策树（decision tree）和基于逻辑的学习，而决策树学习技术由于简单易用，到今天仍是最常用的机器学习技术之一

7，通常我们把分类错误的样本占样本总数的比例称为错误率，即如果在m个样本中有a个样本分类错误，则错误率E=a/m，相应的，1-a/m称为“精度”，即“精度=1-错误率”。

8，泛化误差可以由偏差，方差，以及噪声之和，偏差 方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的

9，有监督学习(supervised learning)： 训练集有类别标记(class label)
无监督学习(unsupervised learning)： 无类别标记(class label)
半监督学习（semi-supervised learning)：有类别标记的训练集 + 无标记的训练集

10，机器学习步骤框架：
（1）把数据拆分为训练集和测试集
（2）用训练集和训练集的特征向量来训练算法
（3）用学习来的算法运用在测试集上来评估算法 （可能要设计到调整参数（parameter tuning), 用验证集（validation set）

11，机器学习中分类和预测算法的评估：
准确率
速度
强壮行
可规模性
可解释性

12，监督学习（分类）算法：决策树

13，决策树的优点：直观，便于理解，小规模数据集有效     
决策树的缺点：处理连续变量不好，类别较多时，错误增加的比较快，可规模性一般