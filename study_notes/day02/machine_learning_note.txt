1，监督学习（分类）算法：最近邻规则分类KNN算法

2，最近邻规则分类KNN算法步骤：
     为了判断未知实例的类别，以所有已知类别的实例作为参照
     选择参数K
     计算未知实例与所有已知实例的距离
     选择最近的K个已知实例
     根据少数服从多数的投票法则(majority-voting)
     
3，算法优缺点：
     3.1 算法优点
          简单
          易于理解
          容易实现
          通过对K的选择可具备丢噪音数据的健壮性
          
     3.2 算法缺点          
          需要大量空间储存所有已知实例
          算法复杂度高（需要比较所有已知实例与要分类的实例）
          当其样本分布不平衡时，比如其中一类样本过大（实例数量过多）占主导的时候，新的未知实例容易被归类为这个主导样本，因为这类样本实例的数量过大，但这个新的未知实例实际并木接近目标样本

4，改进版本：
      考虑距离，根据距离加上权重
      比如: 1/d (d: 距离）

5，监督学习（分类）算法：支持向量机（SVM）算法：深度学习（2012）出现之前，SVM被认为机器学习中近十几年来最成功，表现最好的算法

6，SVM寻找区分两类的超平面（hyper plane), 使边际(margin)最大

7，线性可区分(linear separable) 和 线性不可区分 （linear inseparable) 

8，所有坐落在边际的两边的的超平面上的被称作”支持向量(support vectors)"，最大边际距离2/||W||

9，优点：
     9.1 训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以SVM不太容易产生overfitting
     9.2 SVM训练出来的模型完全依赖于支持向量(Support Vectors), 即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果仍然会得到完全一样的模型。
     9.3 一个SVM如果训练得出的支持向量个数比较小，SVM训练出的模型比较容易被泛化。

10，线性不可区分 （linear inseparable) 
     10.1 数据集在空间中对应的向量不可被一个超平面区分开
     10.2 两个步骤来解决：
          10.2.1 利用一个非线性的映射把原数据集中的向量点转化到一个更高维度的空间中
          10.2.2 在这个高维度的空间中找一个线性的超平面来根据线性可分的情况处理