1，监督学习算法：多层向前神经网络(Multilayer Feed-Forward Neural Network)
     1.1 Backpropagation被使用在多层向前神经网络上
     1.2 多层向前神经网络由以下部分组成：
           输入层(input layer), 隐藏层 (hidden layers), 输入层 (output layers)
     1.3 每层由单元(units)组成
     1.4 输入层(input layer)是由训练集的实例特征向量传入
     1.5 经过连接结点的权重(weight)传入下一层，一层的输出是下一层的输入
     1.6 隐藏层的个数可以是任意的，输入层有一层，输出层有一层
     1.7 每个单元(unit)也可以被称作神经结点，根据生物学来源定义
     1.8 以上成为2层的神经网络（输入层不算）
     1.8 一层中加权的求和，然后根据非线性方程转化输出
     1.9 作为多层向前神经网络，理论上，如果有足够多的隐藏层(hidden layers) 和足够大的训练集, 可以模     
          拟出任何方程

2，神经网络可以解决两类问题，既可以解决分类问题，也可以解决回归问题

3，设计神经网络结构
     3.1 使用神经网络训练数据之前，必须确定神经网络的层数，以及每层单元的个数
     3.2 特征向量在被传入输入层时通常被先标准化(normalize）到0和1之间 （为了加速学习过程）
     3.3 离散型变量可以被编码成每一个输入单元对应一个特征值可能赋的值
          比如：特征值A可能取三个值（a0, a1, a2), 可以使用3个输入单元来代表A。
                    如果A=a0, 那么代表a0的单元值就取1, 其他取0；
                    如果A=a1, 那么代表a1的单元值就取1，其他取0，以此类推

     3.4 神经网络即可以用来做分类(classification）问题，也可以解决回归(regression)问题
          3.4.1 对于分类问题，如果是2类，可以用一个输出单元表示（0和1分别代表2类）
                                         如果多余2类，每一个类别用一个输出单元表示
                   所以输出层的单元数量通常等于类别的数量

          3.4.2 没有明确的规则来设计最好有多少个隐藏层
                    3.4.2.1 根据实验测试和误差，以及准确度来实验并改进

4，交叉验证方法(Cross-Validation)

5，Backpropagation算法
     5.1 通过迭代性的来处理训练集中的实例
     5.2 对比经过神经网络后输入层预测值(predicted value)与真实值(target value)之间
     5.3 反方向（从输出层=>隐藏层=>输入层）来以最小化误差(error)来更新每个连接的权重(weight)
     5.4 算法详细介绍
           输入：D：数据集，l 学习率(learning rate)， 一个多层前向神经网络
           输入：一个训练好的神经网络(a trained neural network)

          5.4.1 初始化权重(weights)和偏向(bias): 随机初始化在-1到1之间，或者-0.5到0.5之间，每个单元有一个偏向
          5.4.2 对于每一个训练实例X，执行以下步骤：
                    5.4.2.1： 由输入层向前传送
                    5.4.2.2 根据误差(error)反向传送
                                   对于输出层：
                                   对于隐藏层：
                                   权重更新：
                                   偏向更新：
               5.4.3 终止条件
                         5.4.3.1 权重的更新低于某个阈值
                         5.4.3.2 预测的错误率低于某个阈值
                         5.4.3.3 达到预设一定的循环次数

6，关于非线性转化方程(non-linear transformation function)

sigmoid函数(S 曲线)用来作为activation function:

     6.1 双曲函数(tanh)
     
     6.2 逻辑函数(logistic function)

7，监督学习算法：简单线性回归Regression

8，统计量：描述数据特征
集中趋势衡量：均值（平均数，平均值）（mean）；中位数 （median）: 将数据中的各个数值按照大小顺序排列，居于中间位置的变量；众数 （mode）：数据中出现次数最多的数

离散程度衡量：方差（variance)；标准差 (standard deviation)，即方差开方

9，回归(regression) Y变量为连续数值型(continuous numerical variable)
分类(Classification): Y变量为类别型(categorical variable)

10，简单线性回归(Simple Linear Regression)
     10.1 很多做决定过过程通常是根据两个或者多个变量之间的关系
     10.3 回归分析(regression analysis)用来建立方程模拟两个或者多个变量之间如何关联
     10.4 被预测的变量叫做：因变量(dependent variable), y, 输出(output)
     10.5 被用来进行预测的变量叫做： 自变量(independent variable), x, 输入(input)

11，简单线性回归介绍
     11.1 简单线性回归包含一个自变量(x)和一个因变量(y)
     11.2 以上两个变量的关系用一条直线来模拟
     11.3 如果包含两个以上的自变量，则称作多元回归分析(multiple regression)

12，简单线性回归模型
     12.1 被用来描述因变量(y)和自变量(X)以及偏差(error)之间关系的方程叫做回归模型
     12.2 简单线性回归的模型是:y=B0+B1X+E（E是偏差）

13，简单线性回归方程
                         E(y) = β0+β1x 
         这个方程对应的图像是一条直线，称作回归线
         其中，β0是回归线的截距
                  β1是回归线的斜率  
                  E(y)是在一个给定x值下y的期望值（均值）
14，估计的简单线性回归方程
          ŷ=b0+b1x
     这个方程叫做估计线性方程(estimated regression line)
     其中，b0是估计线性方程的纵截距
               b1是估计线性方程的斜率
               ŷ是在自变量x等于一个给定值的时候，y的估计值

15，关于偏差ε的假定
     15.1 是一个随机的变量，均值为0
     15.2 ε的方差(variance)对于所有的自变量x是一样的
     15.3 ε的值是独立的
     15.4 ε满足正态分布

16，找到一条线，使得sum of squares最小