1，监督学习算法：多元回归分析multiple Regression
与简单线性回归区别(simple linear regression)多个自变量(x)

2，多元回归模型
     y=β0＋β１x1+β2x2+ ... +βpxp+ε
    其中：β0，β１，β2... βp是参数
                 ε是误差值

3，多元回归方程
     E(y)=β0＋β１x1+β2x2+ ... +βpxp

4，估计多元回归方程:
     y_hat=b0＋b１x1+b2x2+ ... +bpxp

    一个样本被用来计算β0，β１，β2... βp的点估计b0, b1, b2,..., bp

5，关于误差的分布
    误差ε是一个随机变量，均值为0
    ε的方差对于所有的自变量来说相等
    所有ε的值是独立的
    ε满足正态分布，并且通过β0＋β１x1+β2x2+ ... +βpxp反映y的期望值

6，监督学习算法：非线性回归：Logistic Regression (逻辑回归)
    处理二值数据，引入Sigmoid函数时曲线平滑化

    根据模型得到预测模型，对预测出的系数，结合Cost函数用梯度下降解法，不断地更新系数，找到合适的系数

    6.1 学习率
    6.2 同时对所有的θ进行更新
    6.3 重复更新直到收敛   

7，衡量两个值的线性相关强度的量（衡量线性回归，如果效果不是特别的好，则可以选择非线性回归算法）：回归中的相关度和R平方值

8，皮尔逊相关系数 (Pearson Correlation Coefficient):
         8.1 衡量两个值线性相关强度的量
         8.2 取值范围 [-1, 1]: 
                    正向相关: >0, 负向相关：<0, 无相关性：=0

9，R平方值:

     9.1定义：决定系数，反应因变量的全部变异能通过回归关系被自变量解释的比例。

     9.2 描述：如R平方为0.8，则表示回归关系可以解释因变量80%的变异。换句话说，如果我们能控制自变量不变，则因变量的变异程度会减少80%

     9.3： 简单线性回归：R^2 = r * r（r就是皮尔逊相关系数）
              多元线性回归：

10，总结：监督学习：分类里的标记y是一个分类的变量；回归里的标记y是一个连续变化的变量
非监督学习：聚类里无类别标记