1，非监督学习算法（无类别标记(class label)）：聚类(clustering)：K-means 算法

2，K-means 算法：

     2.1 Clustering 中的经典算法，数据挖掘十大经典算法之一
     2.2 算法接受参数 k ；然后将事先输入的n个数据对象划分为 k个聚类以便使得所获得的聚类满足：同一
           聚类中的对象相似度较高；而不同聚类中的对象相似度较小。
     2.3 算法思想：
           以空间中k个点为中心进行聚类，对最靠近他们的对象归类。通过迭代的方法，逐次更新各聚类中心     
           的值，直至得到最好的聚类结果
     2.4 算法描述：
          
          （1）适当选择c个类的初始中心；
          （2）在第k次迭代中，对任意一个样本，求其到c各中心的距离，将该样本归到距离最短的中心所在     
                  的类；
          （3）利用均值等方法更新该类的中心值；
          （4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，
                   否则继续迭代。
     2.5 算法流程：
          
          输入：k, data[n];
          （1） 选择k个初始中心点，例如c[0]=data[0],…c[k-1]=data[k-1];
          （2） 对于data[0]….data[n], 分别与c[0]…c[k-1]比较，假定与c[i]差值最少，就标记为i;
          （3） 对于所有标记为i点，重新计算c[i]={ 所有标记为i的data[j]之和}/标记为i的个数；
          （4） 重复(2)(3),直到所有c[i]值的变化小于给定阈值。

3，优点：速度快，简单
缺点：最终结果跟初始点选择相关，容易陷入局部最优，需直到k值

4，非监督学习算法（无类别标记(class label)）：聚类(clustering)：hierarchical clustering 层次聚类

5，假设有N个待聚类的样本，对于层次聚类来说，步骤：
       1、（初始化）把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度；
       2、寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）；
       3、重新计算新生成的这个类与各个旧类之间的相似度；
       4、重复2和3直到所有样本点都归为一类，结束

6， 整个聚类过程其实是建立了一棵树，在建立的过程中，可以通过在第二步上设置一个阈值，当最近的两个类的距离大于这个阈值，则认为迭代可以终止。另外关键的一步就是第三步，如何判断两个类之间的相似度有不少种方法。这里介绍一下三种：
       SingleLinkage：又叫做 nearest-neighbor ，就是取两个类中距离最近的两个样本的距离作为这两个集合的距离，也就是说，最近两个样本之间的距离越小，这两个类之间的相似度就越大。容易造成一种叫做 Chaining 的效果，两个 cluster 明明从“大局”上离得比较远，但是由于其中个别的点距离比较近就被合并了，并且这样合并之后 Chaining 效应会进一步扩大，最后会得到比较松散的 cluster 。

       CompleteLinkage：这个则完全是 Single Linkage 的反面极端，取两个集合中距离最远的两个点的距离作为两个集合的距离。其效果也是刚好相反的，限制非常大，两个 cluster 即使已经很接近了，但是只要有不配合的点存在，就顽固到底，老死不相合并，也是不太好的办法。这两种相似度的定义方法的共同问题就是指考虑了某个有特点的数据，而没有考虑类内数据的整体特点。

       Average-linkage：这种方法就是把两个集合中的点两两的距离全部放在一起求一个平均值，相对也能得到合适一点的结果。

       average-linkage的一个变种就是取两两距离的中值，与取均值相比更加能够解除个别偏离样本对结果的干扰。
