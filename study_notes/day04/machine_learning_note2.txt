1，多个隐藏层的神经网络
MultiLayer Perceptions (MLP): 实际是sigmoid neurons, 不是perceptrons

如果图片是64*64, 输入层总共有64*64 = 4096个神经元
如果图片是28*28, 输入层总共有28*28 = 784个神经元

如果输出层只有一个神经元, >0.5说明是, <0.5说明不是

2，FeedForward Network: 神经网络中没有循环, 信息单项向前传递

3，深度学习神经网络中，通过训练，减小w和b的偏差的方法是使用梯度下降算法，不断地更新w和b值

4，Backpropagation算法（反向传递，简称BP）
     4.1 通过迭代性的来处理训练集中的实例
     4.2 对比经过神经网络后输入层预测值(predicted value)与真实值(target value)之间
     4.3 反方向（从输出层=>隐藏层=>输入层）来以最小化误差(error)来更新每个连接的权重(weight)
     4.4 算法详细介绍
           输入：D：数据集，l 学习率(learning rate)， 一个多层前向神经网络
           输出：一个训练好的神经网络(a trained neural network)

          4.4.1 初始化权重(weights)和偏向(bias): 随机初始化在-1到1之间，或者-0.5到0.5之间，每个单元有          
                    一个偏向
          4.4.2 对于每一个训练实例X，执行以下步骤：
                    4.4.2.1： 由输入层向前传送
                    4.4.2.2 根据误差(error)反向传送
          4.4.3 终止条件
                         4.4.3.1 权重的更新低于某个阈值
                         4.4.3.2 预测的错误率低于某个阈值
                         4.4.3.3 达到预设一定的循环次数

5，cross-entropy cost几乎总是比二次cost函数好
如果神经元的方程是线性的, 用二次cost函数 (不会有学习慢的问题)
注意：sigmoid函数并不是线性的

6， 增加训练数据集的量是减少overfitting（过拟合）的途径之一
减小神经网络的规模, 但是更深层更大的网络潜在有更强的学习能力

Regularization减少overfitting

最常见的一种regularization: (weight decay)L2 regularization

Regularization的Cost偏向于让神经网络学习比较小的权重w, 除非第一项的Co明显减少

最终结果在测试集上accuracy达到97.92, 比隐藏层30个神经元提高很多

如果调整优化一下参数 用 学习率=0.1, λ=5.0, 只需要30个epoch, 准确率就超过了98%,达到了98.04%

加入regularization不仅减小了overfitting, 还对避免陷入局部最小点 (local minimum), 更容易重现实验结果

7，为什么Regularization可以减少overfitting?
在神经网络中:
Regularized网络更鼓励小的权重, 小的权重的情况下, x一些随机的变化不会对神经网络的模型造成太大影响, 所以更小可能受到数据局部噪音的影响.

Un-regularized神经网路, 权重更大, 容易通过神经网络模型比较大的改变来适应数据,更容易学习到局部数据的噪音

Regularized更倾向于学到更简单一些的模型

简单的模型不一定总是更好,要从大量数据实验中获得,目前添加regularization可以更好的泛化更多的从实验中得来,理论的支持还在研究之中

8，Dropout:

和L1, L2 regularization非常不同, 不是针对cost函数增加一项,而是对神经网络本身的结构做改变

假设我们有一个神经网络

通常, 我们根据输入的x,正向更新神经网络,算出输出值,然后反向根据backpropagation来更新权重和偏向

但是, dropout不同:

开始, 删除掉隐藏层随机选取的一半神经元


然后, 在这个更改过的神经网络上正向和反向更新, 利用一个mini-batch

然后, 恢复之前删除过的神经元, 重新随机选择一半神经元删除, 正向, 反向, 更新w,b

重复此过程

最后,学习出来的神经网络中的每个神经元都是在只有一半神经元的基础上学习的, 当所有神经元被恢复后, 为了补偿, 我们把隐藏层的所有权重减半


9，为什么dropout可以减少overfitting?

假设我们对于同一组训练数据, 利用不同的神经网络来训练, 训练完成之后, 求输出的平均值, 这样可以减少overfitting

Dropout和这个是同样的道理, 每次扔到一半隐藏层的神经元, 相当于我们在不同的神经网络上训练了

减少了神经元的依赖性, 也就是每个神经元不能依赖于某个或者某几个其他神经元, 迫使神经网聚学习更加和其他神经元联合起来的更加健硕的特征

介绍dropout的文章, 对于以前MNIST最高的accuracy是98.4%, 利用dropout, 提高到98.7%

10，比如: MNIST, 一个隐藏层有800个神经元的网络, 98.4%, 人工增加数据后, 达到98.9%, 发明了一下人工会改变图像的模拟方法进一步增大训练集, 准确率达到了 99.3%

增大时, 要模拟现实世界中这种数据可能出现的变化, 来概括更广

11，均值=0, 方差=1, 标准正太分布

12，softmax：计算输出层激活神经元a的函数，类似sigmoid，只是用在输出层
第一步 (和之前sigmoid一样): 
第二步: (和之前sigmoid不同): softmax函数
分母是把所有神经元的输入值加起来

当最后一行z增大时,a也随之增大,其他a随之减小

事实上, 其他a减小的值总是刚好等于a4增加的值, 总和为1不变

Softmax的输出每个值都是大于等于0, 而且总和等于1
所以, 可以认为是概率分布

容易描述, 可以认为输出的是分类等于每个可能分类标签的概率(如 P(a(x)) = 0.8 for MNIST)

如果输出层是sigmod层, 不能默认输出总和为1, 所以不能轻易描述为概率分布

13，通过改变随机生成的权重区间，可以改善学习慢的问题[0,1/n_in)] n_in是输入层神经元的个数
从正态分布均值=0, 标准差差等于 1/sqrt(n_in)

不再使用均值为0，方差为1的方法