1，池化层有压缩数据和参数的量，减小过拟合；
2，RNN不仅仅能够处理序列输出，也能得到序列输出，这里序列指的是向量的序列；
3，RNN学习出来的是程序（状态机），不是函数；
4，PyTorch 采用了动态计算图（dynamic computational graph）结构，而不是大多数开源框架，比如 TensorFlow、Caffe、CNTK、Theano 等采用的静态计算图。
5，jupyter中的%matplotlib inline 的作用是：这是一条专门的命令，用于通知笔记本将 matplotlib 图表直接显示在浏览器中。
6，将新的anaconda虚拟环境，与jupyter内核相关联的步骤：
conda install -n 环境名称 ipykernel（给新的虚拟环境装ipykernel）
python -m ipykernel install --user --name 环境名称 --display-name "Python (环境名称)"

7，对于 softmax 分类，使用了一个含 C 个神经元的网络，其中每个神经元对应一个可能的输出类别。（所有的输出类别的概率和为1，选出概率最大的那个，作为本次样本，预测的输出）

8，梯度下降法是一种致力于找到函数极值点的算法。前面介绍过，所谓 “ 学习 ” 便是改进模型参数，以便通过大量训练步骤将损失最小化。有了这个概念，将梯度下降法应用于寻找损失函数的极值点便构成了依据输入数据的模型学习。

9，梯度下降法无法区分迭代终止时到底是到达了全局最小点还是局部极小点，后者往往只在一个很小的邻域内为最优。

10，可通过将权值随机初始化来改善上述问题。请记住，权值的初值是手工指定的。通过使用随机值，可以增加从靠近全局最优点附近开始下降的机会。

11，前馈时，从输入开始，逐一计算每个隐含层的输出，直到输出层。然后开始计算导数，并从输出层经各隐含层逐一反向传播。为了减少计算量，还需对所有已完成计算的元素进行复用。这便是反向传播算法名称的由来。

12，TensorFlow 中包含了少量不同的优化算法，它们都是基于计算梯度的梯度下降法。到底哪一种算法更有效，具体取决于输入数据的形状，以及所要求解的问题的特点。

13，在计算机视觉中，卷积的价值体现在对输入（本例中为图像）降维的能力上。

14，一个神经网络架构要成为 CNN ，必须至少包含一个卷积层（ tf.nn.conv2d ）。单层 CNN 的一种实际用途是检测边缘。

15，激励函数应该满足的两个特性：
1），该函数应是单调 的 ，这样输出便会随着输入的增长而增长，从而使利用梯度下降法寻找局部极值点成为可能。
2），该函数应是可微分的 ，以保证该函数定义域内的任意一点上导数都存在，从而使得梯度下降法能够正常使用来自这类激活函数的输出。